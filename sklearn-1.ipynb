{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 监督学习的目标\n",
    "利用一组带有标签的数据，学习从输入到输出的映射，然后将这种映射关系应用到未知数据上，达到分类或回归的目的。\n",
    "* 分类：当输出是离散的，学习任务为分类任务。\n",
    "* 回归：当输出是连续的，学习任务为回归任务。\n",
    "## 分类学习\n",
    "* 输入：一组有标签的训练数据(也称观察和评估)，标签表明了这些数据（观察）的所署类别。\n",
    "* 输出：分类模型根据这些训练数据，训练自己的模型参数，学习出一个适合这组数据的分类器，当有新数据（非训练数据）需要进行类别判断，就可以将这组新数据作为输入送给学好的分类器进行判断。\n",
    "### 分类学习-评价\n",
    "* 训练集(training set):顾名思义用来训练模型的已标注数据，用来建立模型，发现规律。\n",
    "* 测试集(testing set):也是已标注数据，通常做法是将标注隐藏，输送给训练好的模型，通过结果与真实标注进行对比，评估模型的学习能力。\n",
    "* 训练集/测试集的划分方法：根据已有标注数据，随机选出一部分数据（70%）数据作为训练数据，余下的作为测试数据，此外还有交叉验证法，自助法用来评估分类模型。\n",
    "### 分类学习-评价标准\n",
    "* 精确率：精确率是针对我们预测结果而言的，（以二分类为例）它表示的是预测为正的样本中有多少是真正的正样本。那么预测为正就有两种可能了，一种就是把正类预测为正类(TP)，另一种就是把负类预测为正类(FP)，也就是：$P=\\frac{TP}{TP+FP}$\n",
    "* 召回率：是针对我们原来的样本而言的，它表示的是样本中的正例有多少被预测正确了。那也有两种可能，一种是把原来的正类预测成正类(TP)，另一种就是把原来的正类预测为负类(FN)，也就是:$R=\\frac{TP}{TP+FN}$\n",
    "### SKlearn中的分类学习\n",
    "与聚类算法被统一封装在sklearn.cluster模块不同，sklearn库中的分类算法并未被统一封装在一个子模块中，因此对分类算法的import方式各有不同。\n",
    "Sklearn提供的分类函数包括：\n",
    "* k近邻（knn）\n",
    "* 朴素贝叶斯（naivebayes），\n",
    "* 支持向量机（svm），\n",
    "* 决策树 （decision tree）\n",
    "* 神经网络模型（Neural networks）等\n",
    "* 这其中有线性分类器，也有非线性分类器。\n",
    "### 分类算法中的应用\n",
    "* 金融：贷款是否批准进行评估\n",
    "* 医疗诊断：判断一个肿瘤是恶性还是良性\n",
    "* 欺诈检测：判断一笔银行的交易是否涉嫌欺诈\n",
    "* 网页分类：判断网页的所属类别，财经或者是娱乐？\n",
    "## 回归分析\n",
    "回归：统计学分析数据的方法，目的在于了解两个或多个变数间是否相关、研究其相关方向与强度，并建立数学模型以便观察特定变数来预测研究者感兴趣的变数。回归分析可以帮助人们了解在自变量变化时因变量的变化量。一般来说，通过回归分析我们可以由给出的自变量估计因变量的条件期望。\n",
    "### SKlearn中的回归分析\n",
    "Sklearn提供的回归函数主要被封装在两个子模块中，分别是**sklearn.linear_model**和**sklearn.preprocessing**。\n",
    "* sklearn.linear_modlel封装的是一些线性函数，线性回归函数包括有：\n",
    "1. 普通线性回归函数（ LinearRegression ）\n",
    "2. 岭回归（Ridge）\n",
    "3. Lasso（Lasso）\n",
    "* 非线性回归函数，如多项式回归（PolynomialFeatures）则通过sklearn.preprocessing子模块进行调用\n",
    "### 回归分析应用\n",
    "回归方法适合对一些带有时序信息的数据进行预测或者趋势拟合，常用在金融及其他涉及时间序列分析的领域：\n",
    "* 股票趋势预测\n",
    "* 交通流量预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本分类模型\n",
    "## K近邻分类器(KNN)\n",
    "KNN：通过计算待分类数据点，与已有数据集中的所有数据点的距离。取距离最小的前K个点，根据“少数服从多数“的原则，将这个数据点划分为出现次数最多的那个类别。\n",
    "### SKlearn中的K近邻分类器\n",
    "在sklearn库中，可以使用**sklearn.neighbors.KNeighborsClassifier**创建一个K近邻分类器，主要参数有：\n",
    "* n_neighbors：用于指定分类器中K的大小(默认值为5，注意与kmeans的区别)\n",
    "* weights：设置选中的K个点对分类结果影响的权重（默认值为平均权重“uniform”，可以选择“distance”代表越近的点权重越高，或者传入自己编写的以距离为参数的权重计算函数）\n",
    "* algorithm：设置用于计算临近点的方法，因为当数据量很大的情况下计算当前点和所有点的距离再选出最近的k各点，这个计算量是很费时的，所以（选项中有ball_tree、kd_tree和brute，分别代表不同的寻找邻居的优化算法，默认值为auto，根据训练数据自动选择）\n",
    "### KNN的使用经验\n",
    "在实际使用时，我们可以使用所有训练数据构成特征 X 和标签 y，使用fit() 函数进行训练。在正式分类时，通过一次性构造测试集或者一个一个输入样本的方式，得到样本对应的分类结果。有关K 的取值：\n",
    "* 如果较大，相当于使用较大邻域中的训练实例进行预测，可以减小估计误差，但是距离较远的样本也会对预测起作用，导致预测错误。\n",
    "* 相反地，如果 K 较小，相当于使用较小的邻域进行预测，如果邻居恰好是噪声点，会导致过拟合。\n",
    "* 一般情况下，K 会倾向选取较小的值，并使用交叉验证法选取最优 K 值。\n",
    "## 决策树\n",
    "决策树是一种树形结构的分类器，通过顺序询问分类点的属性决定分类点最终的类别。通常根据特征的信息增益或其他指标，构建一颗决策树。在分类时，只需要按照决策树中的结点依次进行判断，即可得到样本所属类别。\n",
    "### SKlearn中的决策树\n",
    "在sklearn库中，可以使用**sklearn.tree.DecisionTreeClassifier**创建一个决策树用于分类，其主要参数有：\n",
    "* criterion ：用于选择属性的准则，可以传入“gini”代表基尼系数，或者“entropy”代表信息增益。\n",
    "* max_features ：表示在决策树结点进行分裂时，从多少个特征中选择最优特征。可以设定固定数目、百分比或其他标准。它的默认值是使用所有特征个数。\n",
    "### 决策树本质\n",
    "本质上是寻找一种对特征空间上的划分，旨在构建一个训练数据拟合的好，并且复杂度小的决策树。在实际使用中，需要根据数据情况，调整DecisionTreeClassifier类中传入的参数，比如选择合适的criterion，设置随机变量等。\n",
    "## 朴素贝叶斯\n",
    "朴素贝叶斯分类器是一个以贝叶斯定理为基础的多分类的分类器。对于给定数据，首先基于特征的条件独立性假设，学习输入输出的联合概率分布，然后基于此模型，对给定的输入x，利用贝叶斯定理求出后验概率最大的输出y。<br/>\n",
    "> $p ( A|B) =\\frac {p ( B|A)·p ( A)}{p ( B )}$\n",
    "### SKlearn中的朴素贝叶斯\n",
    "在sklearn库中，实现了三个朴素贝叶斯分类器，如下表所示：\n",
    "* naive_bayes.GussianNB 高斯朴素贝叶斯\n",
    "* naive_bayes.MultinomialNB 针对多项式模型的朴素贝叶斯分类器\n",
    "* naive_bayes.BernoulliNB 针对多元伯努利模型的朴素贝叶斯分类器\n",
    "区别在于假设某一特征的所有属于某个类别的观测值符合特定分布，如，分类问题的特征包括人的身高，身高符合高斯分布，这类问题适合高斯朴素贝叶斯\n",
    "* 朴素贝叶斯是典型的生成学习方法，由训练数据学习联合概率分布，并求得后验概率分布。\n",
    "* 朴素贝叶斯一般在小规模数据上的表现很好，适合进行多分类任务。\n",
    "#### SKlearn中的高斯朴素贝叶斯\n",
    "在sklearn库中，可以使用sklearn.naive_bayes.GaussianNB创建一个高斯朴素贝叶斯分类器，其参数有：\n",
    "* priors ：给定各个类别的先验概率。如果为空，则按训练数据的实际情况进行统计；如果给定先验概率，则在训练过程中不能更改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial准确率：0.6057007125890737\n",
      "BernoulliNB准确率：0.7672209026128266\n",
      "词汇库数量： 30777\n"
     ]
    }
   ],
   "source": [
    "import re, os, random\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer, CountVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "def train_and_test_data(data_):\n",
    "    # 训练集和测试集的比例为7:3 计算训练集的大小，将数据集总长度的70%转换为整数，以确保索引不超出范围。\n",
    "    filesize = int(0.7 * len(data_))\n",
    "    # 将数据执行切片操作在数据集中分出数据和标签\n",
    "    train_data_ = [each[0] for each in data_[:filesize]]\n",
    "    train_target_ = [each[1] for each in data_[:filesize]]\n",
    "    test_data_ = [each[0] for each in data_[filesize:]]\n",
    "    test_target_ = [each[1] for each in data_[filesize:]]\n",
    "\n",
    "    return train_data_, train_target_, test_data_, test_target_\n",
    "\n",
    "\n",
    "def getwords(doc):\n",
    "    splitter = re.compile('\\\\W*')\n",
    "    words = [s.lower() for s in splitter.split(doc) if len(s) > 2 and len(s) < 20]\n",
    "    # 过滤掉单词中包含数字的单词\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "    with open(r'./stopwords.txt') as f:\n",
    "        stopwords = f.read()\n",
    "    stopwords = stopwords.split('\\n')\n",
    "    stopwords = set(stopwords)\n",
    "    # 过滤掉一些经常出现的单词,例如 a,an,we,the\n",
    "    words = [word for word in words if word not in stopwords]\n",
    "    return set(words)\n",
    "\n",
    "def get_dataset():\n",
    "    data = []\n",
    "    for root, dirs, files in os.walk(r'./mix20_rand700_tokens_cleaned/tokens/neg'):\n",
    "        for file in files:\n",
    "            realpath = os.path.join(root, file)\n",
    "            with open(realpath, errors='ignore') as f:\n",
    "                content = f.read()\n",
    "                # words = getwords(content)  # 提取单词\n",
    "                data.append((content, 'bad'))  # 将单词重新组合为字符串\n",
    "    '''\n",
    "    os.walk()函数用于递归地遍历指定目录（./mix20_rand700_tokens_cleaned/tokens/neg）下的所有文件。\n",
    "    realpath = os.path.join(root, file)组合目录路径和文件名，得到每个文件的完整路径。\n",
    "    open(realpath, errors='ignore')打开文件，忽略编码错误。\n",
    "    f.read()读取文件的全部内容，并将内容与标签 'bad' 作为一个元组加入 data 列表。\n",
    "    '''\n",
    "    for root, dirs, files in os.walk(r'./mix20_rand700_tokens_cleaned/tokens/pos'):\n",
    "        for file in files:\n",
    "            realpath = os.path.join(root, file)\n",
    "            with open(realpath, errors='ignore') as f:\n",
    "                content = f.read()\n",
    "                # words = getwords(content)  # 提取单词\n",
    "                data.append((content, 'good'))  # 将单词重新组合为字符串\n",
    "    \n",
    "    # 使用random.shuffle()随机打乱数据集中的样本顺序，确保训练和测试时样本的随机性，提高模型的泛化能力。\n",
    "    random.shuffle(data) \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "data = get_dataset()\n",
    "train_data, train_target, test_data, test_target = train_and_test_data(data)\n",
    "\n",
    "'''\n",
    "Pipeline是scikit-learn中用于简化机器学习工作流的工具。它将多个步骤串联起来，从而使得数据处理和模型训练一体化。\n",
    "这里定义的Pipeline包含两个步骤：\n",
    "    'vect': 使用TfidfVectorizer进行文本特征提取。\n",
    "        TfidfVectorizer将文本数据转换为TF-IDF矩阵，该矩阵反映了每个单词在文档中的重要性。\n",
    "        TF-IDF是Term Frequency-Inverse Document Frequency的缩写，用于衡量单词在文本集合中的相对重要性。\n",
    "    'clf': 使用MultinomialNB作为分类器。\n",
    "        MultinomialNB是一个朴素贝叶斯分类器，通常用于多项分布的数据，如文本分类。\n",
    "        alpha=1.0表示使用拉普拉斯平滑（也称为加1平滑）来防止零概率问题。\n",
    "'''\n",
    "nbc_mul = Pipeline([('vect', TfidfVectorizer()), ('clf', MultinomialNB(alpha=1.0)), ])\n",
    "nbc_mul.fit(train_data, train_target)\n",
    "predict = nbc_mul.predict(test_data)\n",
    "count = 0\n",
    "for left, right in zip(predict, test_target):\n",
    "    if left == right:\n",
    "        count += 1\n",
    "\n",
    "print('Multinomial准确率：'+ str(count/len(test_target))) # 输出测试集准确率 \n",
    "\n",
    "nbc_ber = Pipeline([('vect', TfidfVectorizer()), ('clf', BernoulliNB(alpha=1.0)), ])\n",
    "nbc_ber.fit(train_data, train_target)\n",
    "predict = nbc_ber.predict(test_data)\n",
    "count = 0\n",
    "for left, right in zip(predict, test_target):\n",
    "    if left == right:\n",
    "        count += 1\n",
    "\n",
    "print('BernoulliNB准确率：'+ str(count/len(test_target)))\n",
    "\n",
    "# MultinomialNB通常适用于多项分布的数据，常用于文本分类，而BernoulliNB适用于二进制/布尔特征。\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(train_data)\n",
    "print('词汇库数量：', len(vectorizer.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TfidfVectorizer 是 scikit-learn 库中用于文本特征提取的重要工具。它的主要作用是将文本数据转换为数值特征矩阵，以便机器学习模型能够处理。TfidfVectorizer 的工作机制基于计算每个单词在文档中的 TF-IDF 值，即 词频-逆文档频率。\n",
    "### TF-IDF 的概念\n",
    "TF-IDF 是一种常用的文本特征提取方法，用于衡量单词在文档集中的重要性。它结合了两个部分：\n",
    "* TF（词频，Term Frequency）：<br/>\n",
    "    定义：在单个文档中，某个单词出现的次数除以文档中单词的总数。<br/>\n",
    "    目的：衡量某个单词在文档中出现的频率。<br/>\n",
    "    公式：$TF(t,d)= \\frac{文档 d 中词语的总数}{词语 t 在文档 d 中出现的次数} $\n",
    "\n",
    "* IDF（逆文档频率，Inverse Document Frequency）：<br/>\n",
    "    定义：衡量一个单词在整个文档集中有多重要。<br/>\n",
    "    目的：降低那些在多个文档中都频繁出现的常见词的权重。<br/>\n",
    "    公式：$IDF(t,D)=log(\\frac{N}{1+包含词语 t 的文档数})$其中，N为文档总数\n",
    "\n",
    "* 计算TF和IDF：<br/>\n",
    "    结合 TF 和 IDF，得到一个词在文档中的重要性分数。</br>\n",
    "    $TF-IDF(t,d,D)=TF(t,d)×IDF(t,D)$\n",
    "\n",
    "### TfidfVectorizer 的实现\n",
    "TfidfVectorizer 将文本数据转换为 TF-IDF 特征矩阵。它的主要步骤包括：\n",
    "1. 文本预处理：\n",
    "    * 将文本转换为小写，移除标点符号等。\n",
    "    * 通过正则表达式或自定义规则进行词语切分。\n",
    "2. 构建词汇表：\n",
    "    * 为文档集中出现的所有单词建立词汇表（vocabulary）。\n",
    "    * 仅保留词汇表中的单词以进行后续计算。\n",
    "3. 计算 TF 和 IDF：\n",
    "    * 对文档集中的每个单词计算 TF 和 IDF。\n",
    "    * 使用词汇表索引将单词映射到特征矩阵。\n",
    "4. 生成 TF-IDF 特征矩阵：\n",
    "    * 结果是一个稀疏矩阵，其中每一行表示一个文档，每一列表示一个词语。\n",
    "    * 矩阵中的值是每个词语在相应文档中的 TF-IDF 分数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.48133417 0.61980538 0.61980538 0.\n",
      "  0.        ]\n",
      " [0.5844829  0.5844829  0.34520502 0.         0.44451431 0.\n",
      "  0.        ]\n",
      " [0.         0.         0.34520502 0.44451431 0.         0.5844829\n",
      "  0.5844829 ]]\n",
      "['fascinating' 'is' 'learning' 'love' 'machine' 'new' 'things']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 文本数据\n",
    "documents = [\n",
    "    \"I love machine learning\",\n",
    "    \"Machine learning is fascinating\",\n",
    "    \"I love learning new things\"\n",
    "]\n",
    "\n",
    "# 创建 TfidfVectorizer 实例\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# 将文档转换为 TF-IDF 特征矩阵\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# 输出特征矩阵\n",
    "print(tfidf_matrix.toarray())\n",
    "\n",
    "# 输出词汇表\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络实现手写识别\n",
    "<p>手写数字识别是一个多分类问题，共有10个分类，每个手写数字图像的类别标签是0~9中的其中一个数。例如下面这三张图片的标签分别是0，1，2。 </p>\n",
    "<p>任务：利用sklearn来训练一个简单的全连接神经网络，即多层感知机（Multilayer perceptron，MLP）用于识别数据集DBRHD的手写数字。</p>\n",
    "\n",
    "### MLP的输入\n",
    "* DBRHD数据集的每个图片是一个由0或1组成的32*32的文本矩阵；\n",
    "* 多层感知机的输入为图片矩阵展开的1*\n",
    "\n",
    "### MLP的输出\n",
    "* MLP输出：“one-hot vectors”\n",
    "* 一个one-hot向量除了某一位的数字是1以外其余各维度数字都是0。\n",
    "* 图片标签将表示成一个只有在第n维度（从0开始）数字为1的10维向量。\n",
    "<p>比如，标签0将表示成[1,0,0,0,0,0,0,0,0,0,0]。即，MLP输出层具有10个神经元。</p>\n",
    "\n",
    "### MLP的结构\n",
    "* MLP的输入与输出层，中间隐藏层的层数和神经元的个数设置都将影响该MLP模型的准确率。\n",
    "* 在本实例中，我们只设置一层隐藏层，在后续实验中比较该隐藏层神经元个数为50、100、200时的MLP效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(activation='logistic', hidden_layer_sizes=(200,),\n",
      "              learning_rate_init=0.0001, max_iter=2000)\n",
      "Total num: 946  Wrong num: 39   WrongRate: 0.0412262156448203\n"
     ]
    }
   ],
   "source": [
    "# from ucimlrepo import fetch_ucirepo \n",
    "import numpy as np     #导入numpy工具包\n",
    "from os import listdir #使用listdir模块，用于访问本地文件\n",
    "from sklearn.neural_network import MLPClassifier \n",
    "'''\n",
    "# fetch dataset \n",
    "pen_based_recognition_of_handwritten_digits = fetch_ucirepo(id=81) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = pen_based_recognition_of_handwritten_digits.data.features \n",
    "y = pen_based_recognition_of_handwritten_digits.data.targets \n",
    "print(X)\n",
    "print(y)\n",
    "# metadata \n",
    "print(pen_based_recognition_of_handwritten_digits.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(pen_based_recognition_of_handwritten_digits.variables) \n",
    "'''\n",
    "def img2vector(fileName): \n",
    "    '''\n",
    "    将32x32的文本文件中的图像数据转换为1x1024的向量。\n",
    "    使用np.zeros([1024], int)创建一个大小为1024的全零向量，用于存放转换后的数据。\n",
    "    通过读取文件中的每一行，并将行中的每个字符（0或1）存放在返回的向量中。\n",
    "    '''   \n",
    "    retMat = np.zeros([1024],int) #定义返回的矩阵，大小为1*1024\n",
    "    fr = open(fileName)           #打开包含32*32大小的数字文件 \n",
    "    lines = fr.readlines()        #读取文件的所有行\n",
    "    for i in range(32):           #遍历文件所有行\n",
    "        for j in range(32):       #并将01数字存放在retMat中     \n",
    "            retMat[i*32+j] = lines[i][j]   \n",
    "    return retMat\n",
    "\n",
    "def readDataSet(path):    \n",
    "    '''\n",
    "    读取给定路径下的所有文件，并将其转换为数据集和标签。\n",
    "    使用np.zeros([numFiles, 1024], int)创建一个矩阵，用于存放所有文件转换后的向量。\n",
    "    使用np.zeros([numFiles, 10])创建一个矩阵，用于存放one-hot格式的标签。\n",
    "    遍历每个文件，通过文件名获取标签，并设置对应位置的one-hot标签为1。\n",
    "    调用img2vector函数将每个文件的内容转换为向量，并存储在dataSet中。\n",
    "    '''\n",
    "    fileList = listdir(path)    #获取文件夹下的所有文件 \n",
    "    numFiles = len(fileList)    #统计需要读取的文件的数目\n",
    "    dataSet = np.zeros([numFiles,1024],int) #用于存放所有的数字文件\n",
    "    hwLabels = np.zeros([numFiles,10])      #用于存放对应的one-hot标签\n",
    "    for i in range(numFiles):   #遍历所有的文件\n",
    "        filePath = fileList[i]  #获取文件名称/路径      \n",
    "        digit = int(filePath.split('_')[0])  #通过文件名获取标签      \n",
    "        hwLabels[i][digit] = 1.0        #将对应的one-hot标签置1\n",
    "        dataSet[i] = img2vector(path +'/'+filePath) #读取文件内容   \n",
    "    return dataSet,hwLabels\n",
    "\n",
    "#read dataSet\n",
    "train_dataSet, train_hwLabels = readDataSet('trainingDigits')\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(200,),\n",
    "                    activation='logistic', solver='adam',\n",
    "                    learning_rate_init = 0.0001, max_iter=2000)\n",
    "'''\n",
    "    使用MLPClassifier创建一个多层感知器分类器，配置如下：\n",
    "        hidden_layer_sizes=(100,)：设置一个隐藏层，包含100个神经元。\n",
    "        activation='logistic'：使用逻辑回归作为激活函数。\n",
    "        solver='adam'：使用Adam优化算法。\n",
    "        learning_rate_init=0.0001：初始化学习率为0.0001。\n",
    "        max_iter=2000：最大迭代次数为2000。\n",
    "'''\n",
    "print(clf)\n",
    "clf.fit(train_dataSet,train_hwLabels)\n",
    "\n",
    "#read  testing dataSet\n",
    "dataSet,hwLabels = readDataSet('testDigits')\n",
    "res = clf.predict(dataSet)   #对测试集进行预测\n",
    "error_num = 0                #统计预测错误的数目\n",
    "num = len(dataSet)           #测试集的数目\n",
    "for i in range(num):         #遍历预测结果\n",
    "    #比较长度为10的数组，返回包含01的数组，0为不同，1为相同\n",
    "    #若预测结果与真实结果相同，则10个数字全为1，否则不全为1\n",
    "    if np.sum(res[i] == hwLabels[i]) < 10: \n",
    "        error_num += 1                     \n",
    "print(\"Total num:\",num,\" Wrong num:\", \\\n",
    "      error_num,\"  WrongRate:\",error_num / float(num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 隐藏层神经元个数影响\n",
    "* 随着隐藏层神经元个数的增加，MLP的正确率持上升趋势；\n",
    "* 大量的隐藏层神经元带来的计算负担与对结果的提升并不对等，因此，如何选取合适的隐藏神经元个数是一个值得探讨的问题。\n",
    "\n",
    "### 最大迭代次数影响\n",
    "* 过小的迭代次数可能使得MLP早停，造成较低的正确率。\n",
    "* 当最大迭代次数>1000时，正确率基本保持不变，这说明MLP在第1000迭代时已收敛，剩余的迭代次数不再进行。\n",
    "* 一般设置较大的最大迭代次数来保证多层感知机能够收敛，达到较高的正确率。\n",
    "\n",
    "### 学习率影响\n",
    "<p>改用随机梯度下降优化算法即将MLPclassifer的参数（ solver=‘sgd’, ），设隐藏层神经元个数为100，最大迭代次数为2000</p>\n",
    "<p>较小的学习率带来了更低的正确率，这是因为较小学习率无法在2000次迭代内完成收敛，而步长较大的学习率使得MLP在2000次迭代内快速收敛到最优解。因此，较小的学习率一般要配备较大的迭代次数以保证其收敛。</p>\n",
    "\n",
    "### 随机梯度下降 (SGD)\n",
    "**基本原理：**\n",
    "- **梯度下降：** SGD的核心思想是通过计算损失函数对模型参数的梯度来更新参数，使得损失函数逐步减小，从而优化模型。它使用训练数据的一个小批量（mini-batch）来计算梯度，而不是整个数据集。\n",
    "- **参数更新公式：**  \n",
    "\n",
    "  $\\theta = \\theta - \\eta \\cdot \\nabla J(\\theta)$\n",
    "\n",
    "  其中，$\\theta$ 是模型参数，$\\eta$ 是学习率，$\\nabla J(\\theta)$ 是损失函数关于参数的梯度。\n",
    "\n",
    "### Adam（自适应矩估计）\n",
    "\n",
    "**基本原理：**\n",
    "- Adam结合了AdaGrad和RMSProp的思想，利用梯度的一阶矩估计（动量）和二阶矩估计（均方根误差）自适应地调整每个参数的学习率。\n",
    "- **参数更新公式：**\n",
    "\n",
    "  1. **计算梯度的一阶矩估计（动量）：**\n",
    "\n",
    "     $m_t = \\beta_1 \\cdot m_{t-1} + (1-\\beta_1) \\cdot \\nabla J(\\theta_t)$\n",
    "\n",
    "     其中，$\\beta_1$ 是一阶矩估计的指数衰减率，通常设为0.9。\n",
    "\n",
    "  2. **计算梯度的二阶矩估计（均方根误差）：**\n",
    "\n",
    "     $v_t = \\beta_2 \\cdot v_{t-1} + (1-\\beta_2) \\cdot (\\nabla J(\\theta_t))^2$\n",
    "\n",
    "     其中，$\\beta_2$ 是二阶矩估计的指数衰减率，通常设为0.999。\n",
    "\n",
    "  3. **校正偏差：**\n",
    "  \n",
    "     $\\hat{m}_t = \\frac{m_t}{1-\\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1-\\beta_2^t}$\n",
    "\n",
    "  4. **更新参数：**\n",
    "\n",
    "     $\\theta_t = \\theta_{t-1} - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\cdot \\hat{m}_t$\n",
    "     \n",
    "\n",
    "     其中，$\\epsilon$ 是一个非常小的数，防止除以零。\n",
    "\n",
    "### 总结\n",
    "* **SGD是一种简单而有效的优化算法**，适用于许多机器学习任务，但可能需要结合动量等改进来提高其收敛性和稳定性。\n",
    "* **Adam是一种更复杂的优化算法**，适用于大多数深度学习任务，特别是在参数规模较大或数据较稀疏的情况下，具有较好的收敛速度和效果。\n",
    "* 选择优化算法时，通常可以根据具体的任务和数据集特点来决定，**Adam由于其良好的适应性和收敛性能**，通常是许多任务的默认选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN实现手写识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num: 946  Wrong num: 10   WrongRate: 0.010570824524312896\n"
     ]
    }
   ],
   "source": [
    "import numpy as np     #导入numpy工具包\n",
    "from os import listdir #使用listdir模块，用于访问本地文件\n",
    "from sklearn import neighbors\n",
    "\n",
    "def img2vector(fileName):    \n",
    "    retMat = np.zeros([1024],int) #定义返回的矩阵，大小为1*1024\n",
    "    fr = open(fileName)           #打开包含32*32大小的数字文件 \n",
    "    lines = fr.readlines()        #读取文件的所有行\n",
    "    for i in range(32):           #遍历文件所有行\n",
    "        for j in range(32):       #并将01数字存放在retMat中     \n",
    "            retMat[i*32+j] = lines[i][j]    \n",
    "    return retMat\n",
    "\n",
    "def readDataSet(path):    \n",
    "    fileList = listdir(path)    #获取文件夹下的所有文件 \n",
    "    numFiles = len(fileList)    #统计需要读取的文件的数目\n",
    "    dataSet = np.zeros([numFiles,1024],int)    #用于存放所有的数字文件\n",
    "    hwLabels = np.zeros([numFiles])#用于存放对应的标签(与神经网络的不同)\n",
    "    for i in range(numFiles):      #遍历所有的文件\n",
    "        filePath = fileList[i]     #获取文件名称/路径   \n",
    "        digit = int(filePath.split('_')[0])   #通过文件名获取标签     \n",
    "        hwLabels[i] = digit        #直接存放数字，并非one-hot向量\n",
    "        dataSet[i] = img2vector(path +'/'+filePath)    #读取文件内容 \n",
    "    return dataSet,hwLabels\n",
    "\n",
    "#read dataSet\n",
    "train_dataSet, train_hwLabels = readDataSet('trainingDigits')\n",
    "knn = neighbors.KNeighborsClassifier(algorithm='kd_tree', n_neighbors=3)\n",
    "knn.fit(train_dataSet, train_hwLabels)\n",
    "\n",
    "#read  testing dataSet\n",
    "dataSet,hwLabels = readDataSet('testDigits')\n",
    "\n",
    "res = knn.predict(dataSet)  #对测试集进行预测\n",
    "error_num = np.sum(res != hwLabels) #统计分类错误的数目\n",
    "num = len(dataSet)          #测试集的数目\n",
    "print(\"Total num:\",num,\" Wrong num:\", \\\n",
    "      error_num,\"  WrongRate:\",error_num / float(num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN实验效果\n",
    "邻居数量K影响分析：设置K为1、3、5、7的KNN分类器，对比他们的实验效果\n",
    "<table><tr><td>邻居数量K</td><td>1</td><td>3</td><td>5</td><td>7</td></tr>\n",
    "<tr><td>错误数量</td><td>12</td><td>10</td><td>19</td><td>24</td></tr>\n",
    "<tr><td>正确率</td><td>0.9873</td><td>0.9894</td><td>0.9799</td><td>0.9746</td></tr></table>\n",
    "K=3时正确率最高，当K>3时正确率开始下降，这是由于当样本为稀疏数据集时（本实例只有946个样本），其第k个邻居点可能与测试点距离较远，因此投出了错误的一票进而影响了最终预测结果。\n",
    "\n",
    "### KNN与MLP对比实验\n",
    "取在上节对不同的隐藏层神经元个数、最大迭代次数、学习率进行的各个对比实验中准确率最高（H）与最差（L）的MLP分类器来进行对比，其各个MLP的参数设置如下：\n",
    "<table><tr><td>MLP代号</td> <td>隐藏层神经元个数</td> <td>最大迭代次数</td> <td>优化方法</td> <td>初始学习率/学习率</td></tr>\n",
    "<tr><td>MLP-YH</td> <td>200</td> <td>2000</td> <td>adam</td> <td>0.0001</td></tr>\n",
    "<tr><td>MLP-YL</td> <td>50</td> <td>2000</td> <td>adam</td> <td>0.0001</td></tr>\n",
    "<tr><td>MLP-DH</td> <td>100</td> <td>2000</td> <td>adam</td> <td>0.0001</td></tr>\n",
    "<tr><td>MLP-DL</td> <td>100</td> <td>500</td> <td>adam</td> <td>0.0001</td></tr>\n",
    "<tr><td>MLP-XH</td> <td>100</td> <td>2000</td> <td>sgd</td> <td>0.1</td></tr>\n",
    "<tr><td>MLP-XL</td> <td>100</td> <td>2000</td> <td>sgd</td> <td>0.0001</td></tr></table>\n",
    "<br/>\n",
    "将效果最好的KNN分类器（K=3）和效果最差的KNN分类器（K=7）与各个MLP分类器作对比如下：\n",
    "<table><tr><td>  </td><td>分类器</td><td>MLP隐藏层神经元个数（MLP-Y）</td><td>\n",
    "MLP迭代次数\n",
    "（MLP-D）</td><td>\n",
    "MLP学习率\n",
    "（MLP-X）</td><td>\n",
    "KNN邻居\n",
    "数量\n",
    "</td></tr>\n",
    "\n",
    "<tr><td>最</td><td>错误数量</td><td>37</td><td>40</td><td>35</td><td>10</td></tr>\n",
    "<tr><td>好</td><td>正确率</td><td>0.9608</td><td>0.9577</td><td>0.9630</td><td>0.9894</td></tr>\n",
    "<tr><td>最</td><td>错误数量</td><td>47</td><td>50</td><td>222</td><td>24</td></tr>\n",
    "<tr><td>差</td><td>正确率</td><td>0.9503</td><td>0.9471</td><td>0.7653</td><td>0.9746</td></tr>\n",
    "</table>\n",
    "\n",
    "### 结论\n",
    "* KNN的准确率远高于MLP分类器，这是由于MLP在小数据集上容易过拟合的原因。\n",
    "* MLP对于参数的调整比较敏感，若参数设置不合理，容易得到较差的分类效果，因此参数的设置对于MLP至关重要。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
