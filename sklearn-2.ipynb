{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 强化学习\n",
    "* 强化学习就是程序或智能体（agent）通过与环境不断地进行交互学习一个从环境到动作的映射，学习的目标就是使累计回报最大化。\n",
    "* 强化学习是一种试错学习，因其在各种状态（环境）下需要尽量尝试所有可以选择的动作，通过环境给出的反馈（即奖励）来判断动作的优劣，最终获得环境和最优动作的映射关系（即策略）。\n",
    "## 马尔可夫决策过程\n",
    "* 马尔可夫决策过程（Markov DecisionProcess）通常用来描述一个强化学习问题\n",
    "* 智能体agent根据当前对环境的观察采取动作获得环境的反馈，并使环境发生改变的循环过程。\n",
    "### MDP 基本元素\n",
    "* $s∈S$:有限状态state集合，s表示某个特定状态；\n",
    "* $a∈A$:有限动作action集合，a表示某个特定动作；\n",
    "* $T(S, a, S’)$~$Pr(s’|s,a)$: 状态转移模型, 根据当前状态s和动作a预测下一个状态s，这里的Pr表示从s采取行动a转移到s’的概率；\n",
    "* $R(s,a)$:表示agent采取某个动作后的即时奖励，它还有 $R(s,a,s’)$,\n",
    "* $R(s)$ 等表现形式；\n",
    "* $Policy π(s)→a$: 根据当前state来产生action，可表现为$a=π(s)$或\n",
    "* $π(a|s) = P（a|s）$，后者表示某种状态下执行某个动作的概率。\n",
    "### 值函数\n",
    "* **状态值函数V** 表示执行策略π能得到的累计折扣奖励：<br/>\n",
    "$𝑽^{𝝅}(𝒔) = 𝑹 (𝒔, 𝒂) + γ\\sum_{𝐬′∈𝐒}𝒑(𝒔′|𝒔, 𝝅(𝒔)) 𝑽^{𝝅} (𝒔′)$\n",
    "* **状态动作值函数Q(s,a)** 表示在状态s下执行动作a能得到的累计折扣奖励：<br/>\n",
    "$𝑸^{𝝅}(𝒔, 𝒂) = 𝑹(𝒔, 𝒂 )+ γ\\sum_{𝐬′∈𝐒}𝒑(𝒔′|𝒔, 𝝅(𝒔)) 𝑸^{𝝅} (𝒔′,𝝅(𝒔′))$\n",
    "### 最优值函数\n",
    "* $𝑽^{*}(𝒔) = 𝒎𝒂𝒙_{𝒂∈𝑨}[𝑹 (𝒔, 𝒂) + γ\\sum_{𝐬′∈𝐒}𝒑(𝒔′|𝒔, 𝒂) 𝑽^{*} (𝒔′)]$\n",
    "* $𝑸^{*}(𝒔, 𝒂) = 𝑹(𝒔, 𝒂 )+ γ\\sum_{𝐬′∈𝐒}𝒑(𝒔′|𝒔, 𝒂) 𝒎𝒂𝒙_{𝒃∈𝑨}𝑸^{*} (𝒔′,𝒃)$\n",
    "### 最优控制\n",
    "* $ 𝝅(𝒔) = 𝒂𝒓𝒈𝒎𝒂𝒙_{𝒂∈𝑨}[𝑹 (𝒔, 𝒂) + γ\\sum_{𝐬′∈𝐒}𝒑(𝒔′|𝒔, 𝒂) 𝑽^{*} (𝒔′)]$\n",
    "* $𝝅 (𝒔) = 𝒂𝒓𝒈𝒎𝒂𝒙_{𝒂∈𝑨 }𝑸∗( 𝒔, 𝒂)$\n",
    "* $𝑽^{∗}( 𝒔) = 𝒎𝒂𝒙_{𝒂∈𝑨 }𝑸∗ (𝒔, 𝒂)$\n",
    "## 蒙特卡洛强化学习\n",
    "* 在现实的强化学习任务中，环境的转移概率、奖励函数往往很难得知，甚至很难得知环境中有多少状态。若学习算法不再依赖于环境建模，则称为免模型学习，蒙特卡洛强化学习就是其中的一种。\n",
    "* 蒙特卡洛强化学习使用多次采样，然后求取平均累计奖赏作为期望累计奖赏的近似。<br/>\n",
    "$<s0,a0,r1,s1,a1,r2,…,sT-1,aT-1,rT,sT>$<br/>\n",
    "* 蒙特卡洛强化学习直接对状态动作值函数Q(s,a)进行估计，每采样一条轨迹，就根据轨迹中的所有“状态-动作”利用下面的公式对来对值函数进行更新。</br>\n",
    "$𝑸(𝒔, 𝒂) = \\frac{𝐐 (𝐬, 𝐚) ∗ 𝐜𝐨𝐮𝐧𝐭 (𝐬, 𝐚) + 𝐑}{𝐜𝐨𝐮𝐧𝐭 (𝐬, 𝐚) +1}$<br/>\n",
    "* 每次采样更新完所有的“状态-动作”对所对应的Q(s,a)，就需要更新采样策略π。但由于策略可能是确定性的，即一个状态对应一个动作，多次采样可能获得相同的采样轨迹，因此需要借助ε贪心策略：</br>\n",
    "$ 𝝅( 𝒔, 𝒂 )=\\left \\{  \\frac{𝒂𝒓𝒈𝒎𝒂𝒙_{𝒂}𝑸(𝒔, 𝒂)    以概率𝜺}{随机从𝑨中选取动作    以概率𝟏 − 𝜺} \\right \\} $<br/>\n",
    "* 蒙特卡洛强化学习算法需要采样一个完整的轨迹来更新值函数，效率较低，此外该算法没有充分利用强化学习任务的序贯决策结构。\n",
    "* Q-learning算法结合了动态规划与蒙特卡洛方法的思想，使得学习更加高效。\n",
    "## Q-learning算法\n",
    "\n",
    "# 深度强化学习\n",
    "## 深度强化学习（DRL）\n",
    "* 传统强化学习：真实环境中的状态数目过多，求解困难。\n",
    "* 深度强化学习：将深度学习和强化学习结合在一起，通过深度神经网络直接学习环境（或观察）与状态动作值函数Q(s,a)之间的映射关系，简化问题的求解。\n",
    "## Deep Q Network（DQN）\n",
    "* Deep Q Network（DQN）：是将神经网络(neural network) 和Q-learning结合，利用神经网络近似模拟函数Q(s,a)，输入是问题的状态（e.g.,图形），输出是每个动作a对应的Q值，然后依据Q值大小选择对应状态执行的动作，以完成控制。\n",
    "* 神经网络的参数：应用监督学习完成\n",
    "* 学习流程：\n",
    "    1. 状态s输入，获得所有动作对应的Q值Q(s,a)；\n",
    "    2. 选择对应Q值最大的动作 a′ 并执行；\n",
    "    3. 执行后环境发生改变，并能够获得环境的奖励r；\n",
    "    4. 利用奖励r更新Q(s, a′ )--强化学习利用新的Q(s, a′ )更新网络参数—监督学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flappy Bird自主学习程序基本框架\n",
    "## 程序基本框架\n",
    "### 程序与模拟器交互\n",
    "训练过程也就是神经网络（agent）不断与游戏模拟器（Environment）进行交互，通过模拟器获得状态，给出动作，改变模拟器中的状态，获得反馈，依据反馈更新策略的过程。\n",
    "### 训练过程\n",
    "训练过程过程主要分为以下三个阶段：\n",
    "1. 观察期（OBSERVE）：程序与模拟器进行交互，随机给出动作，获取模拟器中的状态，将状态转移过程存放在D（Replay Memory）中；\n",
    "2. 探索期（EXPLORE）：程序与模拟器交互的过程中，依据ReplayMemory中存储的历史信息更新网络参数，并随训练过程降低随机探索率ε;\n",
    "3. 训练期（TRAIN）：ε已经很小，不再发生改变，网络参数随着训练过程不断趋于稳定。\n",
    "### 整体框架—观察期\n",
    "1. 打开游戏模拟器，不执行跳跃动作，获取游戏的初始状态\n",
    "2. 根据ε贪心策略获得一个动作（由于神经网络参数也是随机初始化的，在本阶段参数也不会进行更新，所以统称为随机动作），并根据迭代次数减小ε的大小\n",
    "3. 由模拟器执行选择的动作，能够返回新的状态和反馈奖励\n",
    "4. 将上一状态s，动作a，新状态s‘，反馈r组装成（s，a，s’，r）放进Replay Memory中用作以后的参数更新\n",
    "5. 根据新的状态s‘，根据ε贪心策略选择下一步执行的动作，周而复始，直至迭代次数到达探索期\n",
    "### 整体框架—探索期\n",
    "探索期与观察期的唯一区别在于会根据抽样对网络参数进行更新。\n",
    "1. 迭代次数达到一定数目，进入探索期，根据当前状态s，使用ε贪心策略选择一个动作（可以是随机动作或者由神经网络选择动作），并根据迭代次数减小ε的值\n",
    "2. 由模拟器执行选择的动作，能够返回新的状态和反馈奖励\n",
    "3. 将上一状态s，动作a，新状态s‘，反馈r组装成（s，a，s’，r）放进Replay Memory中用作参数更新\n",
    "4. 从Replay Memory中抽取一定量的样本，对神经网络的参数进行更新\n",
    "5. 根据新的状态s‘，根据ε贪心策略选择下一步执行的动作，周而复始，直至迭代次数到达训练期\n",
    "### 整体框架—训练期\n",
    "* 迭代次数达到一定数目，进入训练期，本阶段跟探索期的过程相同，只是在迭代过程中不再修改ε的值\n",
    "### 模拟器\n",
    "游戏模拟器：使用Python的Pygame模块完成的FlappyBird游戏程序，为了配合训练过程，在原有的游戏程序基础上进行了修改。\n",
    "链接：https://github.com/sourabhv/FlapPyBird\n",
    "### 动作选择模块\n",
    "为ε贪心策略的简单应用，以概率ε随机从动作空间A中选择动作，以1-ε概率依靠神经网络的输出选择动作\n",
    "## 深度神经网络-CNN\n",
    "* DQN：用卷积神经网络对游戏画面进行特征提取，这个步骤可以理解为对状态的提取。\n",
    "* CNN-卷积核：这里的卷积核指的就是移动中 $3*3$ 大小的矩阵。\n",
    "* 卷积操作：使用卷积核与数据进行对应位置的乘积并加和，不断移动卷积核生成卷积后的特征。\n",
    "* 池化操作：对卷积的结果进行操作。最常用的是最大池化操作，即从卷积结果中挑出最大值，如选择一个 $2*2$ 大小的池化窗口。\n",
    "* 卷积神经网络：把Image矩阵中的每个元素当做一个神经元，那么卷积核就相当于输入神经元和输出神经元之间的链接权重，由此构建而成的网络被称作卷积神经网络。\n",
    "## 实现\n",
    "1. 对采集的4张原始图像进行预处理，得到 $80*80*4$ 大小的矩阵；\n",
    "2. 使用32个 $8*8*4$ 大小步长4的卷积核对以上矩阵进行卷积，得到 $20*20*32$ 大小的矩阵；<br/>\n",
    "注：在tensorflow中使用4维向量表示卷积核 $[输入通道数，高度，宽度，输出通道数]$ ，对应于上面的 $[4,8,8,32]$ ，可以理解为32个 $8*8*4$ 大小的卷积核；\n",
    "3. 对以上矩阵进行不重叠的池化操作，池化窗口为 $2*2$ 大小，步长为2，得到 $10*10*32$ 大小的矩阵；\n",
    "4. 使用64个 $4*4*32$ 大小步长为2的卷积核对以上矩阵进行卷积，得到 $5*5*64$ 的矩阵；\n",
    "5. 使用64个 $3*3*64$ 大小步长为1的卷积核对以上矩阵进行卷积，\n",
    "得到$5*5*64$的矩阵；\n",
    "6. 将输出的 $5*5*64$ 大小的数组进行reshape，得到 $1*1600$ 大小的矩阵；\n",
    "7. 在之后添加一个全连接层，神经元个数为512；\n",
    "8. 最后一层也是一个全连接层，神经元个数为2，对应的是就是两个动作的动作值函数；\n",
    "* 通过获得输入s，神经网络就能够：\n",
    "1. 输出 $Q(s,a1)$ 和 $Q(s,a2)$ 比较两个值的大小，就能够评判采用动作a1和a2的优劣，从而选择要采取的动作\n",
    "2. 在选择并执行完采用的动作后，模拟器会更新状态并返回回报值，然后将这个状态转移过程存储进D，进行采样更新网络参数。\n",
    "* 网络参数更新\n",
    "1. D中抽取更新使用的样本；\n",
    "2. 利用神经网络计算 $𝒎𝒂𝒙_{𝒂′}𝑸(𝒔𝟐, 𝒂′) $和$𝑸_{𝒐𝒍𝒅} (𝒔𝟏 , 𝒂𝟏)$ ；\n",
    "3. 计算 $ 𝑸_{𝒏𝒆𝒘}(𝒔_{𝟏}, 𝒂_{𝟏})$，并通过 $𝑸_{𝒏𝒆𝒘} 𝒔_{𝟏}, 𝒂_{𝟏}$和$𝑸_{𝒐𝒍𝒅} (𝒔_{𝟏} , 𝒂_{𝟏})$ 差值更新网络参数。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
